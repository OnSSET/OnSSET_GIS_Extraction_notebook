{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "\n",
    "These are the functions needed in order to run the extraction notebook. **Please do not edit this file as it will break the code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geopandas as gpd   \n",
    "import os\n",
    "import fiona\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import rasterio\n",
    "from geojson import Feature, Point, FeatureCollection\n",
    "import rasterio.fill\n",
    "from shapely.geometry import shape, mapping\n",
    "import json\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox, Listbox\n",
    "from rasterio.features import geometry_mask\n",
    "from osgeo import gdal\n",
    "import datetime\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import scipy.spatial\n",
    "from scipy.stats import mode\n",
    "from rasterio.windows import from_bounds, Window\n",
    "import pyogrio\n",
    "import traceback\n",
    "from exactextract import exact_extract\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "root.attributes(\"-topmost\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zonal stats - method exact_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zonal_stats_exact(name, gdf, method='sum', raster_path=''):\n",
    "\n",
    "    if raster_path == '':\n",
    "        messagebox.showinfo('OnSSET', 'Select the {} map'.format(name))\n",
    "        raster_path=filedialog.askopenfilename(filetypes = ((\"rasters\",\"*.tif\"),(\"all files\",\"*.*\")))\n",
    "\n",
    "    try:\n",
    "        gdf.sort_values(by=['id'], inplace=True)\n",
    "    \n",
    "        with rasterio.open(raster_path) as r:\n",
    "    \n",
    "            results = exact_extract(r, gdf, '{}={}'.format(name, method), include_cols='id', output='pandas')\n",
    "            results.sort_values(by=['id'], inplace=True)\n",
    "            \n",
    "            gdf[name] = results[name]\n",
    "    \n",
    "        print('Processing finished:', datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        return gdf, raster_path\n",
    "    except rasterio.RasterioIOError as e:\n",
    "        print('Could not process {}, layer was not selected or not in the correct format'.format(name))\n",
    "        return [], []\n",
    "    except Exception as e:\n",
    "        print('Error occured: ')\n",
    "        print(traceback.print_exc())\n",
    "        return [], []\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing elevation and slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_elevation_and_slope(name, method, clusters, workspace, crs, raster_path=''):\n",
    "    \n",
    "    if raster_path == '':\n",
    "        messagebox.showinfo('OnSSET', 'Select the {} map'.format(name))\n",
    "        raster_path=filedialog.askopenfilename(filetypes = ((\"rasters\",\"*.tif\"),(\"all files\",\"*.*\")))\n",
    "\n",
    "    clusters, raster_path = zonal_stats_exact(name, clusters, 'mean', raster_path)\n",
    "\n",
    "    gdal.Warp(os.path.join(workspace, \"dem.tif\"), raster_path, dstSRS=crs)\n",
    "\n",
    "    def calculate_slope(DEM):\n",
    "        gdal.DEMProcessing(os.path.join(workspace, 'slope.tif'), DEM, 'slope', options=gdal.DEMProcessingOptions(slopeFormat='percent'))\n",
    "        with rasterio.open(os.path.join(workspace, 'slope.tif')) as dataset:\n",
    "            slope=dataset.read(1)\n",
    "        return slope\n",
    "\n",
    "    slope = calculate_slope(os.path.join(workspace, \"dem.tif\"))\n",
    "\n",
    "    slope = rasterio.open(os.path.join(workspace, 'slope.tif'))\n",
    "    gdal.Warp(os.path.join(workspace , 'slope_4326.tif'), slope.name, dstSRS='EPSG:4326')\n",
    "    slope_4326 = rasterio.open(os.path.join(workspace, 'slope_4326.tif'))\n",
    "\n",
    "    clusters, slope_path = zonal_stats_exact('Slope', clusters, 'max', slope_4326.name)\n",
    "    \n",
    "    #print(datetime.datetime.now())\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting admin 1 boundary name to clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def admin_1(name, admin, crs, workspace, clusters, admin_1_path='', admin_col_name=''):\n",
    "    try:\n",
    "        if admin_1_path == '':\n",
    "            messagebox.showinfo('OnSSET', 'Select the admin 1 boundaries')\n",
    "            admin_1_path = filedialog.askopenfilename(filetypes = ((\"vector\",[\"*.shp\", \"*.gpkg\", \"*.geojson\"]),(\"all files\",\"*.*\")))\n",
    "            \n",
    "            admin_1 = gpd.read_file(admin_1_path)\n",
    "    \n",
    "            messagebox.showinfo('OnSSET', 'Select the column which contains the Admin 1 level names')\n",
    "            options = admin_1.columns.tolist()\n",
    "            admin_col_name = dropdown_popup(options)\n",
    "        else:\n",
    "            admin_1 = gpd.read_file(admin_1_path)\n",
    "\n",
    "        clusters_2 = clusters.copy()\n",
    "\n",
    "        clusters_support = clusters_2[['id', 'geometry']].to_crs({'init': \"EPSG:4326\"})\n",
    "\n",
    "        # Apply spatial join \n",
    "        try:\n",
    "            cluster_support_2 = gpd.sjoin(clusters_support, admin_1[[\"geometry\", admin_col_name]], op='intersects').drop(['index_right'], axis=1)\n",
    "        except:\n",
    "            cluster_support_2 = gpd.sjoin(clusters_support, admin_1[[\"geometry\", admin_col_name]], predicate='intersects').drop(['index_right'], axis=1)\n",
    "        #group_by_id = cluster_support_2.groupby([\"id\"]).sum().reset_index()\n",
    "        clusters_2 = pd.merge(clusters_2, cluster_support_2[['id', admin_col_name]], on='id', how = 'left')\n",
    "        clusters_2.rename(columns = {admin_col_name:'Admin_1'}, inplace = True)\n",
    "\n",
    "        print('Processing finished:', datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        clusters_2.drop_duplicates(subset =\"id\", keep = \"first\", inplace = True)\n",
    "        return clusters_2, admin_1_path\n",
    "        \n",
    "    except fiona.errors.DriverError as e:\n",
    "        print('Could not process Admin 1, layer was not selected or not in the correct format')\n",
    "        return [], []\n",
    "    except pyogrio.errors.DataSourceError:\n",
    "        print('Could not process Admin 1, layer was not selected or not in the correct format')\n",
    "        return [], []\n",
    "    except ValueError as e:\n",
    "        print('Could not process Admin 1. Check the coordinate system and that there is data in the study area')\n",
    "        print(e)\n",
    "        return [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def preparing_for_vectors(workspace, clusters, crs):   \n",
    "    #clusters.crs = {'init' :'epsg:4326'}\n",
    "    clusters = clusters.to_crs({ 'init': crs}) \n",
    "    clusters.drop_duplicates(subset =\"id\", keep = \"first\", inplace = True)\n",
    "    points = clusters.copy()\n",
    "    if (points.geometry[0].type == 'Polygon') or (points.geometry[0].type == 'MultiPolygon'):\n",
    "        points[\"geometry\"] = points[\"geometry\"].centroid\n",
    "    points.to_file(os.path.join(workspace, 'clusters_cp.shp'), driver='ESRI Shapefile')\n",
    "    print('Processing finished:', datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_lines(name, admin, crs, workspace, clusters, lines_path=''):\n",
    "\n",
    "    if lines_path == '':\n",
    "        messagebox.showinfo('OnSSET', 'Select the {} data'.format(name))\n",
    "        lines_path=filedialog.askopenfilename(filetypes = ((\"vector\",[\"*.shp\", \"*.gpkg\", \"*.geojson\"]),(\"all files\",\"*.*\")))\n",
    "    \n",
    "    try:\n",
    "        #messagebox.showinfo('OnSSET', 'Select the ' + name + ' data')\n",
    "        #lines_path = filedialog.askopenfilename(filetypes = ((\"vector\",[\"*.shp\", \"*.gpkg\", \"*.geojson\"]),(\"all files\",\"*.*\")))\n",
    "        lines=gpd.read_file(lines_path)\n",
    "    \n",
    "        lines_clip = gpd.clip(lines, admin)\n",
    "        #lines_clip.crs = {'init' :'epsg:4326'}  # ToDo remove???\n",
    "        lines_proj=lines_clip.to_crs(crs)\n",
    "        lines_proj = gpd.GeoDataFrame(lines_proj['geometry'].explode()).reset_index()\n",
    "    \n",
    "        lines_proj.to_file(os.path.join(workspace, name + \"_proj.shp\"), driver='ESRI Shapefile')\n",
    "        with fiona.open(os.path.join(workspace, name + \"_proj.shp\")) as line:\n",
    "            firstline = line.next()\n",
    "    \n",
    "            schema = {'geometry' : 'Point', 'properties' : {'id' : 'int'},}\n",
    "            with fiona.open(os.path.join(workspace, name + \"_proj_points.shp\"), \"w\", \"ESRI Shapefile\", schema) as output:\n",
    "                for lines in line:\n",
    "                    #if lines[\"geometry\"] != None:\n",
    "                    first = shape(lines['geometry'])\n",
    "                    length = first.length\n",
    "                    for distance in range(0,int(length),100):\n",
    "                        point = first.interpolate(distance)\n",
    "                        output.write({'geometry' :mapping(point), 'properties' : {'id':1}})\n",
    "\n",
    "                        \n",
    "        with fiona.open(os.path.join(workspace, name + \"_proj_points.shp\")) as lines_f, fiona.open(os.path.join(workspace, 'clusters_cp.shp')) as points:\n",
    "            lines = gpd.read_file(os.path.join(workspace, name + \"_proj.shp\"))\n",
    "        \n",
    "            geoms1 = [shape(feat[\"geometry\"]) for feat in lines_f]\n",
    "            s1 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms1]\n",
    "            s1_arr = np.array(s1)\n",
    "        \n",
    "            geoms2 = [shape(feat[\"geometry\"]) for feat in points]\n",
    "            s2 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms2]\n",
    "            s2_arr = np.array(s2)\n",
    "        \n",
    "            def do_kdtree(combined_x_y_arrays,points):\n",
    "                mytree = scipy.spatial.cKDTree(combined_x_y_arrays)\n",
    "                dist, indexes = mytree.query(points)\n",
    "                return dist, indexes\n",
    "        \n",
    "            def vector_overlap(vec, settlementfile, column_name):\n",
    "                vec.drop_duplicates(vec.columns.difference([\"geometry\"]), keep=\"first\", inplace=True)\n",
    "                try:\n",
    "                    a = gpd.sjoin(settlementfile, vec, op = 'intersects')\n",
    "                except:\n",
    "                    a = gpd.sjoin(settlementfile, vec, predicate = 'intersects')\n",
    "                a[column_name + '2'] = 0\n",
    "                return a  \n",
    "        \n",
    "            results1, results2 = do_kdtree(s1_arr,s2_arr)\n",
    "    \n",
    "        z=results1.tolist()\n",
    "\n",
    "        clusters_2 = clusters.copy()\n",
    "        \n",
    "        clusters_2[name+'Dist'] = z\n",
    "        clusters_2[name+'Dist'] = clusters_2[name+'Dist']/1000\n",
    "    \n",
    "        a = vector_overlap(lines, clusters_2, name+'Dist')\n",
    "    \n",
    "        clusters_2 = pd.merge(left = clusters_2, right = a[['id',name+'Dist2']], on='id', how = 'left')\n",
    "        clusters_2.drop_duplicates(subset =\"id\", keep = \"first\", inplace = True) \n",
    "    \n",
    "        clusters_2.loc[clusters_2[name+'Dist2'] == 0, name+'Dist'] = 0\n",
    "    \n",
    "        del clusters_2[name+'Dist2']\n",
    "        print('Processing finished:', datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "        # Clean up temporary shapefile and associated files\n",
    "        base_path = os.path.join(workspace, name + \"_proj\")\n",
    "        for ext in [\".shp\", \".shx\", \".dbf\", \".prj\", \".cpg\"]:\n",
    "            try:\n",
    "                os.remove(base_path + ext)\n",
    "            except FileNotFoundError:\n",
    "                pass  # File might not exist; skip it\n",
    "\n",
    "        # Clean up temporary shapefile and associated files\n",
    "        base_path = os.path.join(workspace, name + \"_proj_points\")\n",
    "        for ext in [\".shp\", \".shx\", \".dbf\", \".prj\", \".cpg\"]:\n",
    "            try:\n",
    "                os.remove(base_path + ext)\n",
    "            except FileNotFoundError:\n",
    "                pass  # File might not exist; skip it\n",
    "        \n",
    "        return clusters_2, lines_path\n",
    "    except fiona.errors.DriverError as e:\n",
    "        print('Could not process ' + '{}'.format(name) + ', layer was not selected or not in the correct format')\n",
    "        return [], []\n",
    "    except pyogrio.errors.DataSourceError:\n",
    "        print('Could not process ' + '{}'.format(name) + ', layer was not selected or not in the correct format')\n",
    "        return [], []\n",
    "    #except ValueError as e:\n",
    "    #    print('Could not process  ' + '{}'.format(name) + '. Check the coordinate system and that there is data in the study area')\n",
    "    #    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def processing_points(name, admin, crs, workspace, clusters, mg_filter, points_path=''):\n",
    "    \n",
    "    if points_path == '':\n",
    "        messagebox.showinfo('OnSSET', 'Select the {} data'.format(name))\n",
    "        points_path=filedialog.askopenfilename(filetypes = ((\"vector\",[\"*.shp\", \"*.gpkg\", \"*.geojson\"]),(\"all files\",\"*.*\")))\n",
    "    \n",
    "    try:\n",
    "        #messagebox.showinfo('OnSSET', 'Select the ' + name + ' data')\n",
    "        #points_path = filedialog.askopenfilename(filetypes = ((\"vector\",[\"*.shp\", \"*.gpkg\", \"*.geojson\"]),(\"all files\",\"*.*\"))) \n",
    "        points=gpd.read_file(points_path)\n",
    "        if mg_filter:\n",
    "            points['umgid'] = range(0, len(points))\n",
    "            points_post = points\n",
    "    \n",
    "        points_clip = gpd.clip(points, admin)\n",
    "        # points_clip.crs = {'init' :'epsg:4326'}\n",
    "        points_proj=points_clip.to_crs(crs)\n",
    "        points_proj = gpd.GeoDataFrame(points_proj['geometry'].explode()).reset_index()\n",
    "            \n",
    "        points_proj.to_file(os.path.join(workspace, name + \"_proj.shp\"), driver='ESRI Shapefile', mode=\"w\")\n",
    "    \n",
    "        with fiona.open(os.path.join(workspace, name + \"_proj.shp\")) as points_f, fiona.open(os.path.join(workspace, 'clusters_cp.shp')) as points2:\n",
    "            points = gpd.read_file(os.path.join(workspace, name + \"_proj.shp\"))\n",
    "        \n",
    "            geoms1 = [shape(feat[\"geometry\"]) for feat in points_f]\n",
    "            s1 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms1]\n",
    "            s1_arr = np.array(s1)\n",
    "            \n",
    "            geoms2 = [shape(feat[\"geometry\"]) for feat in points2]\n",
    "            s2 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms2]\n",
    "            s2_arr = np.array(s2)\n",
    "    \n",
    "        def do_kdtree(combined_x_y_arrays,points):\n",
    "            mytree = scipy.spatial.cKDTree(combined_x_y_arrays)\n",
    "            dist, indexes = mytree.query(points)\n",
    "            return dist, indexes\n",
    "    \n",
    "        def vector_overlap(vec, settlementfile, column_name):\n",
    "            vec.drop_duplicates(vec.columns.difference([\"geometry\"]), keep='first', inplace=True)\n",
    "            try:\n",
    "                a = gpd.sjoin(settlementfile, vec, op = 'intersects')\n",
    "            except TypeError:\n",
    "                a = gpd.sjoin(settlementfile, vec, predicate = 'intersects')\n",
    "            a[column_name + '2'] = 0\n",
    "            return a  \n",
    "    \n",
    "        results1, results2 = do_kdtree(s1_arr,s2_arr)\n",
    "    \n",
    "        z=results1.tolist()\n",
    "\n",
    "        clusters_2 = clusters.copy()\n",
    "        \n",
    "        clusters_2[name+'Dist'] = z\n",
    "        clusters_2[name+'Dist'] = clusters_2[name+'Dist']/1000.\n",
    "        if mg_filter:\n",
    "            z2 = results2.tolist()\n",
    "            clusters_2['umgid'] = z2\n",
    "    \n",
    "        a = vector_overlap(points, clusters_2, name+'Dist')\n",
    "\n",
    "        clusters_2 = pd.merge(left = clusters_2, right = a[['id', name+'Dist2']], on='id', how = 'left')\n",
    "        clusters_2.drop_duplicates(subset =\"id\", keep = \"first\", inplace = True) \n",
    "    \n",
    "        clusters_2.loc[clusters_2[name+'Dist2'] == 0, name+'Dist'] = 0\n",
    "        \n",
    "        if mg_filter:\n",
    "            cols = points_post.columns.tolist()\n",
    "            try:\n",
    "                cols.remove('id')\n",
    "            except ValueError:\n",
    "                pass\n",
    "            try:\n",
    "                cols.remove('geometry')\n",
    "            except ValueError:\n",
    "                pass\n",
    "    \n",
    "            try:\n",
    "                clusters_2 = pd.merge(clusters_2, points_post[['umgid', 'name']], on='umgid', how = 'left')\n",
    "            except:\n",
    "                clusters_2 = pd.merge(clusters_2, points_post[['umgid']], on='umgid', how = 'left')\n",
    "            #clusters = pd.merge(clusters, points_post[['umgid', 'name', \"MV_network\", \"MG_type\"]], on='umgid', how = 'left')\n",
    "            #clusters.rename(columns = {'name':'MGName',\n",
    "            #                           'MV_network':'MGMVstatus',\n",
    "            #                           'MG_type':'MGType'}, inplace = True)\n",
    "    \n",
    "        del clusters_2[name+'Dist2']\n",
    "        if mg_filter:\n",
    "            del clusters_2['umgid']\n",
    "        print('Processing finished:', datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        clusters_2.drop_duplicates(subset =\"id\", keep = \"first\", inplace = True)\n",
    "\n",
    "        # Clean up temporary shapefile and associated files\n",
    "        base_path = os.path.join(workspace, name + \"_proj\")\n",
    "        for ext in [\".shp\", \".shx\", \".dbf\", \".prj\", \".cpg\"]:\n",
    "            try:\n",
    "                os.remove(base_path + ext)\n",
    "            except FileNotFoundError:\n",
    "                pass  # File might not exist; skip it\n",
    "        \n",
    "        return clusters_2, points_path\n",
    "    except fiona.errors.DriverError as e:\n",
    "        print('Could not process ' + '{}'.format(name) + ', layer was not selected or not in the correct format')\n",
    "        return [], []\n",
    "    except pyogrio.errors.DataSourceError:\n",
    "        print('Could not process ' + '{}'.format(name) + ', layer was not selected or not in the correct format')\n",
    "        return [], []\n",
    "    except ValueError as e:\n",
    "        print('Could not process ' + '{}'.format(name) + '. Check the coordinate system and that there is data in the study area')\n",
    "        print(e)\n",
    "        return [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing hydro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def processing_hydro(admin, crs, workspace, clusters, points, hydropowervalue, \n",
    "                     hydropowerunit):\n",
    "\n",
    "    points_clip = gpd.clip(points, admin)\n",
    "    # points_clip.crs = {'init' :'epsg:4326'}\n",
    "    points_proj=points_clip.to_crs(crs)\n",
    "    points_proj = points_proj.explode(index_parts=False, ignore_index=True)\n",
    "\n",
    "    points_proj.to_file(os.path.join(workspace, \"HydropowerDist_proj.shp\"), driver='ESRI Shapefile')\n",
    "    \n",
    "    with fiona.open(os.path.join(workspace, \"HydropowerDist_proj.shp\")) as points_f, fiona.open(os.path.join(workspace, 'clusters_cp.shp')) as points2:\n",
    "        points = gpd.read_file(os.path.join(workspace, \"HydropowerDist_proj.shp\"))\n",
    "    \n",
    "        geoms1 = [shape(feat[\"geometry\"]) for feat in points_f]\n",
    "        s1 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms1]\n",
    "        s1_arr = np.array(s1)\n",
    "        \n",
    "        geoms2 = [shape(feat[\"geometry\"]) for feat in points2]\n",
    "        s2 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms2]\n",
    "        s2_arr = np.array(s2)\n",
    "    \n",
    "        mytree = scipy.spatial.cKDTree(s1_arr)\n",
    "        dist, indexes = mytree.query(s2_arr)\n",
    "            \n",
    "    def vector_overlap(vec, settlementfile, column_name):\n",
    "        vec.drop_duplicates(vec.columns.difference([\"geometry\"]), keep='first', inplace=True)\n",
    "        try:\n",
    "            a = gpd.sjoin(settlementfile, vec, op = 'intersects')\n",
    "        except:\n",
    "            a = gpd.sjoin(settlementfile, vec, predicate = 'intersects')\n",
    "        a[column_name + '2'] = 0\n",
    "        return a  \n",
    "\n",
    "    z1=dist.tolist()\n",
    "    z2=indexes.tolist()\n",
    "\n",
    "    clusters_2 = clusters.copy()\n",
    "    \n",
    "    clusters_2['HydropowerDist'] = z1\n",
    "    clusters_2['HydropowerDist'] = clusters_2['HydropowerDist']/1000\n",
    "    clusters_2['HydropowerFID'] = z2\n",
    "    \n",
    "    z3 = []\n",
    "    for s in indexes:\n",
    "        z3.append(points[hydropowervalue][s])\n",
    "        \n",
    "    clusters_2['Hydropower'] = z3\n",
    "    \n",
    "    x = hydropowerunit\n",
    "    \n",
    "    if x == 'MW':\n",
    "        clusters_2['Hydropower'] = clusters_2['Hydropower']*1000\n",
    "    elif x == 'kW':\n",
    "        clusters_2['Hydropower'] = clusters_2['Hydropower']\n",
    "    else:\n",
    "        clusters_2['Hydropower'] = clusters_2['Hydropower']/1000\n",
    "\n",
    "    a = vector_overlap(points, clusters_2, 'HydropowerDist')\n",
    "\n",
    "    clusters_2 = pd.merge(left = clusters_2, right = a[['id','HydropowerDist2']], on='id', how = 'left')\n",
    "    clusters_2.drop_duplicates(subset =\"id\", keep = \"first\", inplace = True) \n",
    "\n",
    "    clusters_2.loc[clusters_2['HydropowerDist2'] == 0, 'HydropowerDist'] = 0\n",
    "\n",
    "    del clusters_2['HydropowerDist2']\n",
    "    print('Processing finished:', datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "    # Clean up temporary shapefile and associated files\n",
    "    base_path = os.path.join(workspace, \"HydropowerDist_proj\")\n",
    "    for ext in [\".shp\", \".shx\", \".dbf\", \".prj\", \".cpg\"]:\n",
    "        try:\n",
    "            os.remove(base_path + ext)\n",
    "        except FileNotFoundError:\n",
    "            pass  # File might not exist; skip it\n",
    "    \n",
    "    return clusters_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hydro(admin, crs, workspace, clusters):\n",
    "    try:\n",
    "        messagebox.showinfo('OnSSET', 'Select the Hydropower data')\n",
    "        hydro_path = filedialog.askopenfilename(title = \"Select Hydro map\", filetypes = ((\"vector\",[\"*.shp\", \"*.gpkg\", \"*.geojson\"]),(\"all files\",\"*.*\")))\n",
    "        hydro=gpd.read_file(hydro_path)\n",
    "\n",
    "        messagebox.showinfo('OnSSET', 'Select the column which describes the hydropower POWER potential in each location')\n",
    "        options = hydro.columns.tolist()\n",
    "        hydropower = dropdown_popup(options)\n",
    "\n",
    "        messagebox.showinfo('OnSSET', 'Select the UNIT of the power potential')\n",
    "        options=['W', 'kW', 'MW']\n",
    "        hydrounit = dropdown_popup(options) \n",
    "\n",
    "        print(hydropower)\n",
    "        print(hydrounit)\n",
    "\n",
    "        out = processing_hydro(admin, crs, workspace, clusters, hydro, hydropower, hydrounit)\n",
    "\n",
    "        return out, hydro_path\n",
    "    \n",
    "    except fiona.errors.DriverError as e:\n",
    "        print('Could not process Hydro points, layer was not selected')\n",
    "        return [], []\n",
    "    except pyogrio.errors.DataSourceError:\n",
    "        print('Could not process Hydro points, layer was not selected')\n",
    "        return [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hydro_bulk(admin, hydro, hydropower, hydrounit, crs, workspace, clusters):\n",
    "    try:\n",
    "        out = processing_hydro(admin, crs, workspace, clusters, hydro, hydropower, hydrounit)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    except fiona.errors.DriverError as e:\n",
    "        print('Could not process Hydro points, layer was not selected')\n",
    "        return []\n",
    "    except pyogrio.errors.DataSourceError:\n",
    "        print('Could not process Hydro points, layer was not selected')\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print('Error occured: ')\n",
    "        print(traceback.print_exc())\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def conditioning(clusters, workspace, popunit):\n",
    "    clusters = clusters.to_crs({ 'init': 'epsg:4326'}) \n",
    "\n",
    "    clusters = clusters.rename(columns={\"NightLight\": \"NightLights\", popunit : \"Pop\",})\n",
    "\n",
    "    if \"Area\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"Area\": \"GridCellArea\"})\n",
    "        \n",
    "    if \"Night Lightsmean\" in clusters:\n",
    "        try:\n",
    "            del clusters['NightLights']\n",
    "            clusters = clusters.rename(columns={\"NightLightmean\": \"NightLights\"})\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "\n",
    "    if \"Solar GHImean\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"Solar GHImean\": \"GHI\"})\n",
    "        \n",
    "    if \"TravelTime\" in clusters:\n",
    "        clusters[\"TravelTime\"] = clusters[\"TravelTime\"]/60\n",
    "        clusters = clusters.rename(columns={\"TravelTime\": \"TravelHours\"})\n",
    "    elif \"TravelHour\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"TravelHour\": \"TravelHours\"})\n",
    "        \n",
    "    if \"Wind speedmean\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"Wind speedmean\": \"WindVel\"})\n",
    "    \n",
    "    if \"Residentia\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"Residentia\": \"ResidentialDemandTierCustom\"})\n",
    "    elif \"Custom Demandmean\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"Custom Demandmean\": \"ResidentialDemandTierCustom\"})\n",
    "    elif \"CustomDemand\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"CustomDemand\": \"ResidentialDemandTierCustom\"})\n",
    "    else:\n",
    "        clusters[\"ResidentialDemandTierCustom\"] = 0\n",
    "        \n",
    "    if \"Urban_Demand_Indexmean\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"Urban_Demand_Indexmean\": \"ResidentialDemandTierCustomUrban\"})\n",
    "    #else:\n",
    "    #    clusters[\"ResidentialDemandTierCustomUrban\"] = 0\n",
    "        \n",
    "    if \"Rural_Demand_Indexmean\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"Rural_Demand_Indexmean\": \"ResidentialDemandTierCustomRural\"})\n",
    "    #else:\n",
    "    #    clusters[\"ResidentialDemandTierCustomRural\"] = 0\n",
    "    \n",
    "    if \"Substation\" in clusters:\n",
    "        clusers = clusters.rename(columns={\"Substation\": \"SubstationDist\"})\n",
    "    elif \"SubstationDist\" not in clusters:\n",
    "        clusters[\"SubstationDist\"] = 99999\n",
    "\n",
    "    if \"CurrentHVL\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"CurrentHVL\": \"Existing_HVDist\"})\n",
    "    \n",
    "    if \"CurrentMVL\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"CurrentMVL\": \"Existing_MVDist\"})\n",
    "    \n",
    "    if \"PlannedHVL\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"PlannedHVL\": \"Planned_HVDist\"})\n",
    "    \n",
    "    if \"PlannedMVL\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"PlannedMVL\": \"Planned_MVDist\"})\n",
    "\n",
    "    if \"Existing_HVDist\" in clusters:\n",
    "        try:\n",
    "            del clusters[\"CurrentHVLineDist\"]\n",
    "        except:\n",
    "            pass\n",
    "        clusters = clusters.rename(columns={\"Existing_HVDist\": \"CurrentHVLineDist\"})\n",
    "    elif \"Existing_HVDist\" not in clusters and \"CurrentHVLineDist\" not in clusters:    \n",
    "        clusters[\"CurrentHVLineDist\"] = 99999\n",
    "        \n",
    "    if \"Planned_HVDist\" in clusters:\n",
    "        try:\n",
    "            del clusters[\"PlannedHVLineDist\"]\n",
    "        except:\n",
    "            pass\n",
    "        clusters = clusters.rename(columns={\"Planned_HVDist\": \"PlannedHVLineDist\"})\n",
    "        clusters[\"PlannedHVLineDist\"] = np.minimum(clusters[\"CurrentHVLineDist\"], clusters[\"PlannedHVLineDist\"])\n",
    "    elif \"Planned_HVDist\" not in clusters and \"PlannedHVLineDist\" not in clusters:    \n",
    "        clusters[\"PlannedHVLineDist\"] = clusters[\"CurrentHVLineDist\"]\n",
    "    elif \"Planned_HVDist\" not in clusters and clusters[\"PlannedHVLineDist\"].min() == 99999:\n",
    "        clusters[\"PlannedHVLineDist\"] = clusters[\"CurrentHVLineDist\"]\n",
    "        \n",
    "    if \"Existing_MVDist\" in clusters:\n",
    "        try:\n",
    "            del clusters[\"CurrentMVLineDist\"]\n",
    "        except:\n",
    "            pass\n",
    "        clusters = clusters.rename(columns={\"Existing_MVDist\": \"CurrentMVLineDist\"})\n",
    "    elif \"Existing_MVDist\" not in clusters and \"CurrentMVLineDist\" not in clusters:    \n",
    "        clusters[\"CurrentMVLineDist\"] = 99999\n",
    "        \n",
    "    if \"Planned_MVDist\" in clusters:\n",
    "        try:\n",
    "            del clusters[\"PlannedMVLineDist\"]\n",
    "        except:\n",
    "            pass\n",
    "        clusters = clusters.rename(columns={\"Planned_MVDist\": \"PlannedMVLineDist\"})\n",
    "        clusters[\"PlannedMVLineDist\"] = np.minimum(clusters[\"CurrentMVLineDist\"], clusters[\"PlannedMVLineDist\"])\n",
    "    elif \"Planned_MVDist\" not in clusters and \"PlannedMVLineDist\" not in clusters:    \n",
    "        clusters[\"PlannedMVLineDist\"] = clusters[\"CurrentMVLineDist\"]\n",
    "    elif \"Planned_MVDist\" not in clusters and clusters[\"PlannedMVLineDist\"].min() == 99999:\n",
    "        clusters[\"PlannedMVLineDist\"] = clusters[\"CurrentMVLineDist\"]\n",
    "\n",
    "    if \"RoadsDist\" in clusters:\n",
    "        try:\n",
    "            del clusters[\"RoadDist\"]\n",
    "        except:\n",
    "            pass\n",
    "        clusters = clusters.rename(columns={\"RoadsDist\": \"RoadDist\"})\n",
    "    elif \"RoadDist\" not in clusters:\n",
    "        clusters[\"RoadDist\"] = 99999\n",
    "        \n",
    "    if \"Transforme\" in clusters:\n",
    "        try:\n",
    "            del clusters[\"TransformerDist\"]\n",
    "        except:\n",
    "            pass\n",
    "        clusters = clusters.rename(columns={\"Transforme\": \"TransformerDist\"})\n",
    "    elif \"Service TransformerDist\" in clusters:\n",
    "        try:\n",
    "            del clusters[\"TransformerDist\"]\n",
    "        except:\n",
    "            pass\n",
    "        clusters = clusters.rename(columns={\"Service TransformerDist\": \"TransformerDist\"})\n",
    "    elif \"TransformerDist\" not in clusters:\n",
    "        clusters[\"TransformerDist\"] = 99999\n",
    "\n",
    "    if \"Hydropower\" not in clusters:\n",
    "        clusters[\"Hydropower\"] = 0\n",
    "        \n",
    "    if \"Hydropow_1\" in clusters:\n",
    "        try:\n",
    "            del clusters[\"HydropowerDist\"]\n",
    "        except:\n",
    "            pass\n",
    "        clusters = clusters.rename(columns={\"Hydropow_1\": \"HydropowerDist\"})\n",
    "    elif 'HydropowerDist' not in clusters:\n",
    "        clusters[\"HydropowerDist\"] = 99999\n",
    "        \n",
    "    if \"Hydropow_2\" in clusters:\n",
    "        try:\n",
    "            del clusters[\"HydropowerFID\"]\n",
    "        except:\n",
    "            pass\n",
    "        clusters = clusters.rename(columns={\"Hydropow_2\": \"HydropowerFID\"})\n",
    "    elif \"HydropowerFID\" not in clusters:\n",
    "        clusters[\"HydropowerFID\"] = 0\n",
    "    \n",
    "    if \"IsUrban\" not in clusters:\n",
    "        clusters[\"IsUrban\"] = 0    \n",
    "        \n",
    "    if \"HealthDema\" not in clusters:\n",
    "        clusters[\"HealthDemand\"] = 0     \n",
    "    else:\n",
    "        try:\n",
    "            del clusters[\"HealthDemand\"]\n",
    "        except:\n",
    "            pass\n",
    "        clusters = clusters.rename(columns={\"HealthDema\": \"HealthDemand\"})    \n",
    "    if \"HF_kWh\" in clusters:\n",
    "        clusters[\"HealthDemand\"] = clusters[\"HF_kWh\"]\n",
    "        \n",
    "    if \"EducationD\" not in clusters:\n",
    "        clusters[\"EducationDemand\"] = 0     \n",
    "    else:\n",
    "        try:\n",
    "            del clusters[\"EducationDemand\"]\n",
    "        except:\n",
    "            pass\n",
    "        clusters = clusters.rename(columns={\"EducationD\": \"EducationDemand\"})\n",
    "    if \"EF_kWh\" in clusters:\n",
    "        clusters[\"EducationDemand\"] = clusters[\"EF_kWh\"]\n",
    "        \n",
    "    if \"AgriDemand\" not in clusters:\n",
    "        clusters[\"AgriDemand\"] = 0  \n",
    "        \n",
    "    if \"Commercial\" not in clusters:\n",
    "        clusters[\"CommercialDemand\"] = 0\n",
    "    else:\n",
    "        try:\n",
    "            del clusters[\"CommercialDemand\"]\n",
    "        except:\n",
    "            pass\n",
    "        clusters = clusters.rename(columns={\"Commercial\": \"CommercialDemand\"})\n",
    "        \n",
    "    if (\"MiniGridDist\" not in clusters) and (\"MGDist\" not in clusters):\n",
    "        clusters[\"MGDist\"] = 99999\n",
    "    elif \"MiniGridDist\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"MiniGridDist\": \"MGDist\"})\n",
    "    \n",
    "    clusters[\"X_deg\"] = clusters.geometry.centroid.x\n",
    "    \n",
    "    clusters[\"Y_deg\"] = clusters.geometry.centroid.y\n",
    "    \n",
    "    #clusters[\"Commercial_Multiplier\"] = 0\n",
    "    \n",
    "    # del clusters[\"geometry\"]\n",
    "    \n",
    "    cols = clusters.columns.tolist()\n",
    "    cols.remove('geometry')\n",
    "    \n",
    "    clusters_2 = clusters[cols]\n",
    "\n",
    "    if 'ElecPop' not in clusters_2:\n",
    "        clusters_2['ElecPop'] = 0\n",
    "    \n",
    "    #clusters.to_file(workspace + r\"\\GEP-OnSSET_InputFile.shp\", driver='ESRI Shapefile')\n",
    "    clusters_2.to_csv(os.path.join(workspace, \"GEP-OnSSET_InputFile.csv\"), index=False)\n",
    "\n",
    "    #for a in ['Existing_HV', 'Existing_HV_proj', 'Existing_HV_proj_points', ]:\n",
    "    \n",
    "    print('Processing finished:', datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    print(\"The extraction file is now ready for review & use in the workspace directory as 'OnSSET_InputFile.csv'!\")\n",
    "    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropdown_popup(options):\n",
    "    selected_value = None\n",
    "\n",
    "    def on_select():\n",
    "        nonlocal selected_value\n",
    "        selected_value = variable.get()\n",
    "        window.destroy()\n",
    "\n",
    "    # Use Toplevel instead of creating a new Tk instance\n",
    "    window = tk.Toplevel()\n",
    "    window.title(\"Choose an option\")\n",
    "\n",
    "    variable = tk.StringVar(window)\n",
    "    variable.set(options[0])  # default value\n",
    "\n",
    "    dropdown = tk.OptionMenu(window, variable, *options)\n",
    "    dropdown.pack(padx=100, pady=10)\n",
    "\n",
    "    button = tk.Button(window, text=\"OK\", command=on_select)\n",
    "    button.pack(pady=20, padx=40)\n",
    "\n",
    "    # Update the window to calculate size\n",
    "    window.update_idletasks()\n",
    "\n",
    "    # Get screen width and height\n",
    "    screen_width = window.winfo_screenwidth()\n",
    "    screen_height = window.winfo_screenheight()\n",
    "\n",
    "    # Get window width and height\n",
    "    window_width = window.winfo_width()\n",
    "    window_height = window.winfo_height()\n",
    "\n",
    "    # Calculate x and y coordinates\n",
    "    x = (screen_width // 2) - (window_width // 2)\n",
    "    y = (screen_height // 2) - (window_height // 2)\n",
    "\n",
    "    # Set geometry\n",
    "    window.geometry(f\"+{x}+{y}\")\n",
    "\n",
    "    # Wait until this window is closed\n",
    "    window.grab_set()\n",
    "    window.wait_window()\n",
    "\n",
    "    return selected_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_pop_clusters():\n",
    "    messagebox.showinfo('OnSSET', 'Select the clusters')\n",
    "    file = filedialog.askopenfilename(filetypes = ((\"vector\",[\"*.shp\", \"*.gpkg\", \"*.geojson\"]),(\"all files\",\"*.*\")))\n",
    "    clusters = gpd.read_file(file)\n",
    "    options = clusters.columns.tolist()\n",
    "    messagebox.showinfo('OnSSET', 'Select the column with population counts in the clusters')\n",
    "    x = dropdown_popup(options)\n",
    "    print('Population column: ' + x)\n",
    "    \n",
    "    return x, clusters, file "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
